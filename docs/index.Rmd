---
title: "<span class='title'>Mixed Models</span>"
subtitle: "<span class='subtitle'>with R!</span>"
author: "<span class='author'>Michael Clark</span>"
institute: "<span class='institute'><br>m-clark.github.io <br> @statsdatasci <br> CSCAR, UM</span>"
date: "<span class='date'>2020-10-21</span>"
css: style.css
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse

background-image: url(https://github.com/m-clark/m-clark.github.io/raw/master/img/Rlogo.svg)

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, crayon.enabled = FALSE) # see https://github.com/hadley/mastering-shiny/issues/132

knitr::opts_chunk$set(
  # code
  echo      = T,
  eval      = F,
  message   = F,
  warning   = F,
  error     = F,
  comment   = NA,
  R.options = list(width = 220),
  # viz
  dev.args  = list(bg = 'transparent'),
  dev       = 'svglite',
  fig.align = 'center',
  out.width = '75%',
  fig.asp   = .75,
  # cache
  cache.rebuild = F,
  cache         = F
)

kable_df = function(data, digits=3, ...) {
  kableExtra::kable(
    data,
    digits = digits,
    format = 'html',
    booktabs = T,
    # longtable = F,
    linesep = "",
    ...,
  ) %>% 
    kableExtra::kable_styling(full_width = F)
}

perc = function(x, digits = 1) paste(rnd(x*100, digits = digits), '%')
```

```{r setup-extra, echo=FALSE, eval=TRUE}
xaringanExtra::use_xaringan_extra(
  c(
    "tile_view",
    "animate_css",
    "tachyons",
    'clipboard',
    'fit_screen',
    'webcam',
    'panelset'
  )
)

xaringanExtra::use_logo(
  image_url = 'https://raw.githubusercontent.com/m-clark/m-clark.github.io/master/img/mc_logo.png',
  link_url = 'https://m-clark.github.io',
  width = '5%',
  position =  xaringanExtra::css_position(bottom = "-3em", left = "1em"),
  exclude_class = c("title-slide")
)

xaringanExtra::use_animate_css()

xaringanExtra::style_share_again(
  share_buttons = c("twitter", "linkedin", "pocket")
)
```



```{r load-packs, include=FALSE, eval=TRUE}
library(tidyverse)
library(plotly)
library(visibly)
library(scico)
library(mixedup)

load('../data/gapminder_recent_g20.RData')
```


---
class: inverse middle center


### **Overview of Mixed Models**

### *More Random Effects*

### *Common Extensions*

### *Issues*

### *Bayesian Approaches*


---
class: inverse middle center animated rollIn rollOut # https://animate.style/


# Getting Started

<br>
<br>
![](img/multilevel1.png)



---
class: inverse smaller50

# Getting Started

.pull-left[
Required for workshop:

```{r eval=FALSE}
install.packages('lme4')
```

Others you have already:

- <span class="pack" style = "">nlme</span>
    - included with base R
    
- <span class="pack" style = "">mgcv</span>
    - included with base R
]

.pull-right[
Other recommended (in general)

- <span class="pack" style = "">glmmTMB</span>

- <span class="pack" style = "">rstanarm</span>
    - Bayesian, requires <span class="pack" style = "">rstan</span>
    
- <span class="pack" style = "">brms</span>   
    - Bayesian, requires <span class="pack" style = "">rstan</span>

Misc

- <span class="pack" style = "">mixedup</span>
    - cleaner results for basic models
    - [https::github.com/m-clark/mixedup/](https::github.com/m-clark/mixedup/) 
- <span class="pack" style = "">lmerTest</span>
- <span class="pack" style = "">merTools</span>
]

---
class: inverse slide-font-66

# Getting Started: A starting point

```{r echo = F, eval=FALSE}
data('gapminder_2019', package = 'noiris')
gapminder_recent_g20 = gapminder_2019 %>% 
  filter(
    year >= 1950,
    country %in% c(
     'Australia',
     'Canada',
     'Saudi Arabia',
     'United States',
     'India',
     'Russia',
     'South Africa',
     'Turkey',
     'Argentina',
     'Brazil',
     'Mexico',
     'France',
     'Germany',
     'Italy',
     'Spain',
     'United Kingdom',
     'China',
     'Indonesia',
     'Japan',
     'South Korea'
    )
  ) %>% 
  mutate(
    year0 = year - min(year),
    decade0 = year0/10
  )

save(gapminder_recent_g20, file = 'data/gapminder_recent_g20.RData')
```


Run the following models and summarize.
- <span class="objclass" style = "">`gapminder_recent_g20`</span> 
    - country level data across time (GDP, life expectancy, etc.)
    - limited to > 1950 and countries in the g20
- Run and use <span class="func" style = "">summary</span> on the models.

```{r eval=F}
load('data/gapminder_recent_g20.RData')
```


```{r eval=TRUE}
library(lme4)

mod_lm   =  lm(lifeExp ~ year0, data = gapminder_recent_g20)
mod_lmer = lme4::lmer(lifeExp ~ year0 + (1|country), data = gapminder_recent_g20)
```

What's the same?

What is different?

---
class: img-slide

<!-- # Getting Started: A starting point -->

```{r eval=TRUE, echo=FALSE, out.width='125%'}


ggplot(gapminder_recent_g20, aes(x = year, y = lifeExp)) +
  geom_point(alpha = .1) +
  geom_smooth(method = 'lm', color = '#542437FF', se = F) +
  labs(subtitle = 'lm') +
  theme_clean()
```

---
class: img-slide

<!-- # Getting Started: A starting point -->

```{r eval=TRUE, echo=FALSE, out.width='125%'}
library(brolgar)

gapminder_ts = as_tsibble(gapminder_recent_g20, key = country, index = year)
 
gapminder_ts %>% 
  arrange(continent) %>% 
  mutate(country = fct_inorder(country)) %>% 
  ggplot(aes(x = year, y = lifeExp, color = country)) +
  # geom_line() +
  geom_point(alpha = .1) +
  geom_smooth(aes(), method = 'lm', se = FALSE) +
  facet_wrap(~country, nrow = 5) +
  scale_color_scico_d() +
  scale_x_continuous(breaks = seq(1900, 2010, by = 10)) +
  scale_y_continuous(breaks = seq(20, 80, by = 20)) +
  guides(color = 'none', x = guide_axis(n.dodge =2)) +
  labs(title = 'lmer') +
  theme_clean() +
  theme(
    axis.text = element_text(size = 6)
  )
```


---
class: inverse slide-font-66

# Getting Started: Terminology

The following describe what we'd generally call:

*<div class="center">mixed models</div>*

- Variance components
- Random intercepts and slopes
- Random effects
- Random coefficients
- Varying coefficients
- Intercepts- and/or slopes-as-outcomes
- Hierarchical linear models
- Multilevel models
- Growth curve models
- Mixed effects models

---
class: inverse

# Getting Started: Terminology

The 'mixed' part comes from a mixture of:
- *fixed effects*
- *random effects*

Perhaps a better way to phrase:
- *general* 
    - Applies to all observations
- *specific*
    - Applies to a particular group

---
class: inverse

# Getting Started: Clustering

Specific effects come from natural clustering

- Repeated measures
- Spatial/geography
- Social/teams
- Items on a test

---
class: inverse

# Getting Started: GPA Data

Data Description


><span class="" style = "font-size: 75%">We'll assess factors predicting college grade point average (GPA).  200 students are assessed for six occasions (each semester for the first three years), so we have observations clustered within students. We have other variables such as job status, sex, and high school GPA.  Some will be in both labeled and numeric form.</span>

```{r}
library(tidyverse)

load('data/gpa.RData')

head(gpa)
```

```{r eval=TRUE, echo = F}
library(tidyverse)

load('../data/gpa.RData')

head(gpa)
```



---
class: inverse

# Getting Started: Standard Regression

<div style = "text-align: center;">Scary formulas! 
<br><br>
<span style = "font-size: 200%; ">ðŸ‘»</span></div>

$$\mathscr{gpa} = b_{\mathrm{intercept}} + b_{\mathrm{occ}}\cdot \mathscr{occasion} + \epsilon$$

We have coefficients ( $b$ ) for the intercept and the effect of time.  The error ( $\epsilon$ ) is assumed to be normally distributed with mean 0 and some standard deviation $\sigma$.

$$\epsilon \sim \mathcal{N}(0, \sigma)$$


---
class: inverse

# Getting Started: Standard Regression

Alternatively:

$$\mathscr{gpa} \sim \mathcal{N}(\mu, \sigma)$$

$$\mu = b_{\mathrm{intercept}} + b_{\mathrm{occ}}\cdot \mathscr{occasion}$$

---
class: inverse

# Getting Started: Standard Regression

Run it!

```{r eval=TRUE}
gpa_lm = lm(gpa ~ occasion, data = gpa)
summary(gpa_lm)
```

---
class: inverse

# Getting Started: Single Random Effect

But the observations are clustered
- Not independent
- Correlated within a cluster

--

Regression by cluster?

- Issues:
    - Not easily summarized with many groups
    - Often little data within each cluster 
    - Overfit
    - Ignores what clusters have in common


---
class: img-slide


Think that standard model is adequate?

```{r echo=FALSE, eval=TRUE, out.width='150%'}
gpa_ts <- as_tsibble(
  gpa,
  key = student,
  index = occasion,
  regular = TRUE
)

gpa_feat = gpa_ts %>% 
  features(gpa, feat_spread)

library(gghighlight)
gpa_ts %>% 
  features(gpa, feat_spread) %>% 
  left_join(gpa_ts) %>% 
  mutate(var_type = NA,
         var_type = case_when(
           var > quantile(var, .95) ~ 'Variable',
           var < quantile(var, .05) ~ 'Constant'
         )) %>% 
  ggplot(aes(x = occasion, y = gpa, color = var_type, group = student)) +
  geom_line() +
  gghighlight(!is.na(var_type), unhighlighted_params = list(size = .1), use_direct_label = F) +
  geom_point(alpha = .5) +
  labs(subtitle = 'Comparing highly variable vs. fairly constant') +
  scale_color_scico_d(end = .6) + 
  # guides(color = guide_colorbar(title = NULL)) +
  theme_clean()
```


---
class: inverse

# Getting Started: Mixed Model

Add a (random) student effect:

$$\mathscr{gpa} = b_{\mathrm{intercept}} + b_{\mathrm{occ}}\cdot \mathscr{occasion} + (\mathrm{effect}_{\mathscr{student}} + \epsilon)$$

Assume the following distribution for it:  


$$\mathrm{effect}_{\mathrm{student}} \sim \mathcal{N}(0, \tau)$$

---
class: inverse

# Getting Started: Mixed Model


Rearrange it!

$$\mathscr{gpa} = (b_{\mathrm{intercept}} + \mathrm{effect}_{\mathscr{student}}) + b_{\mathrm{occ}}\cdot \mathscr{occasion} +  \epsilon$$
$$\mathscr{gpa} = b_{\mathrm{int\_student}} + b_{\mathrm{occ}}\cdot \mathscr{occasion} +  \epsilon$$

$$b_{\mathrm{int\_student}} \sim \mathcal{N}(b_{\mathrm{intercept}}, \tau)$$
<br>
Can now focus on model coefficients
- Rather than as another source of 'error'

---
class: inverse slide-font-66

# Getting Started: Mixed Model

Another way of showing the mixed model: 

*<div style = "text-align:center">multilevel modeling</div>*

Two part regression model
  - one at the observation level
  - one at the student level

$$\mathrm{gpa} = b_{\mathrm{int\_student}} + b_{\mathrm{occ}}\cdot \mathrm{occasion} + \epsilon$$

$$b_{\mathrm{int\_student}} = b_{\mathrm{intercept}} + \mathrm{effect}_{\mathrm{student}}$$

After 'plugging in' the second level part to the first, the model is identical to the previous.


---
class: img-slide

# Getting Started: Application

```{r spaghetti, echo=FALSE, eval=TRUE}
set.seed(1234)
gpa_lm = lm(gpa ~ occasion, data = gpa)
# sample_students = gpa %>% filter(student %in% sample(1:200, 10))
# occasion_sample = gpa$occasion[gpa$student %in% sample_students$student]
# gpa_sample = gpa$gpa[gpa$student %in% sample_students$student]
init = gpa %>%
  modelr::add_predictions(gpa_lm, var = 'all') %>%
  mutate(select = factor(student %in% sample(1:200, 10)),
         sz = c(.5, 1)[select]) %>%
  group_by(student, select) 

init %>%
  ggplot(aes(occasion, gpa)) +
  geom_line(aes(group = student), alpha = .05) +
  geom_line(
    aes(group = student),
    color = '#542437FF',
    data = gpa %>% filter(student %in% sample(student, 10))
  ) +
  geom_smooth(color = '#b20076', method = 'lm', se = F) +
  theme_clean()
```



---
class: inverse code-only

Add a student effect!

```{r gpa-group-lm, echo=FALSE}
gpa_lm_by_group = gpa %>%
  split(.$student) %>%
  map_df( ~ data.frame(t(coef(
    lm(gpa ~ occasion, data = .x)
  )))) %>%
  rename(Intercept = X.Intercept.)

coef_lm = coef(gpa_lm)
```

```{r eval=TRUE, message = T}
gpa_mixed = lmer(gpa ~ occasion + (1 | student), data = gpa)

# summary(gpa_mixed, cor = FALSE) # dont' want FE correlations
mixedup::summarize_model(gpa_mixed, digits = 3)
```

---
class: inverse 

# Getting Started: Application

Fixed effects:

```{r eval=T}
# lme4::fixef(gpa_mixed)
mixedup::extract_fixed_effects(gpa_mixed)
```

Just our standard regression coefficients!
- Interpreted the same

---
class: inverse 

# Getting Started: Application

Variance components:

```{r eval=T}
# lme4::VarCorr(gpa_mixed)
mixedup::extract_vc(gpa_mixed)
```

These are the standard deviation/variance of the random effects.

The residual SD is akin to that in the `lm` result

---
class: inverse slide-font-75

# Getting Started: Application

```{r echo = F, eval=T}
extract_vc(gpa_mixed)
```

The proportion of variance attributable to students: ~`r extract_vc(gpa_mixed, ci_level=0)$var_prop[1]*100`%

Can also be interpreted as a correlation.
- Correlation of observations w/in a student
- Called the *intraclass correlation*
- Assumed constant across observations

We'll come back to this... 
<!-- <img src="img/back.jpg" style="display:inline; margin: 0 auto; width:10%"> -->


---
class: inverse 

# Getting Started: Application

These are the random effects for students:

```{r eval=TRUE}
# lme4::ranef(gpa_mixed)
mixedup::extract_random_effects(gpa_mixed)
```

---
class: inverse 

# Getting Started: Application

These are the random coefficients:
- In this case, intercepts
- coef = intercept + effect

```{r eval=TRUE}
# coef(gpa_mixed)
mixedup::extract_random_coefs(gpa_mixed)
```

---
class: inverse 

# Getting Started: Application

Prediction can include random effects or not.

If they do not, essentially the same as `lm`.

But the random effects will help produce better predictions. ðŸ˜ƒ

```{r eval=T}
predict_lm      = predict(gpa_lm)
predict_no_re   = predict(gpa_mixed, re.form=NA)
predict_with_re = predict(gpa_mixed)
```

---
class: img-slide

```{r eval=T, echo=FALSE}
tibble(
  student = as.character(gpa$student),
  lm = predict_lm,
  `lmer no re` = predict_no_re,
  `lmer with re` = predict_with_re
) %>%
  mutate_if(is.numeric, round, digits=2) %>%
  head(30) %>%
  kable_df()
```

---
class: img-slide slide-font-75

# Getting Started: Application

Which do you think has the better fit? 
- One line for all? Separate for each student?

```{r echo=F, eval=T, out.width='80%'}
init = tibble(
  student = as.character(gpa$student),
  occasion = gpa$occasion,
  gpa = gpa$gpa,
  lm = predict_lm,
  mixed = predict_with_re
) %>%
  filter(student %in% 1:3) %>%
  pivot_longer(lm:mixed, names_to = 'type', values_to = 'prediction')

init %>%
  ggplot(aes(x = occasion)) +
  geom_point(
    aes(y = gpa, color = student),
    size = 3,
    position = position_jitter(width = 0, height = .05),
    alpha = .75,
    data = filter(init, type == 'lm')
  ) +
  geom_line(aes(y = prediction, color = student, group = student), 
    data = filter(init, type == 'lm'),
    color = '#ff5500',
    size = 2
  ) +
  geom_line(aes(y = prediction, color = student, group = student),
            data = filter(init, type == 'mixed')) +
  scale_color_scico_d(begin = .2,
                      end = .9,
                      palette = 'hawaii') +
  labs(subtitle = '') +
  theme_clean()
```

---
class: inverse 

# Getting Started: Cluster level covariates

Thus far we've had only one covariate:
- Time indicator
- Varies with outcome, at observation level

We can have cluster-level covariates also
- Just add as you would any other

```{r}
lmer(gpa ~ occasion + sex + (1|student), gpa)
```

---
class: inverse 

# Getting Started: Exercises


### Sleep

For this exercise, we'll use the sleep study data from the <span class="pack">lme4</span> package.  The following describes it.

> The average reaction time per day for subjects in a sleep deprivation study. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time (in milliseconds) on a series of tests given each day to each subject.

---
class: inverse 

# Getting Started: Exercises

After loading the package, the data can be loaded as follows. 

```{r sleepstudy}
library(lme4)
data("sleepstudy")
```

1. Run a regression with Reaction as the target variable and Days as the predictor. 

2. Run a mixed model with a random intercept for Subject.

3. Interpret the variance components and fixed effects.

---
class: inverse 

# Getting Started: Exercises


Rerun the mixed model with the GPA data adding the cluster level covariate of `sex`, or high school GPA (`highgpa`), or both.  Interpret all aspects of the results.

```{r gpa_cluster, echo=F, eval=FALSE}
gpa_mixed_cluster_level = lmer(gpa ~ occasion + sex + highgpa + (1|student), gpa)
summary(gpa_mixed_cluster_level)
```

What happened to the student variance after adding cluster level covariates to the model?


---
class: inverse middle center

# Moving on...


# [Part 2](https://m-clark.github.io/mixed-models-with-R-workshop/part-2.html)  <span class="" style = "width: 50px; display: inline-block"></span>[Part 3](https://m-clark.github.io/mixed-models-with-R-workshop/part-3.html)  <span class="" style = "width: 50px; display: inline-block"></span>[Part 4](https://m-clark.github.io/mixed-models-with-R-workshop/part-4.html)  <span class="" style = "width: 50px; display: inline-block"></span>[Part 5](https://m-clark.github.io/mixed-models-with-R-workshop/part-5.html)
