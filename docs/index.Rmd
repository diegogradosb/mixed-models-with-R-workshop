---
title: "<span class='title'>Mixed Models</span>"
subtitle: "<span class='subtitle'>with R!</span>"
author: "<span class='author'>Michael Clark</span>"
institute: "<span class='institute'>m-clark.github.io <br> @statsdatasci <br> CSCAR, UM</span>"
date: "<span class='date'>2020-10-21</span>"
css: style.css
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse

background-image: url(https://github.com/m-clark/m-clark.github.io/raw/master/img/Rlogo.svg)

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, crayon.enabled = FALSE) # see https://github.com/hadley/mastering-shiny/issues/132

knitr::opts_chunk$set(
  # code
  echo      = T,
  eval      = F,
  message   = F,
  warning   = F,
  error     = F,
  comment   = NA,
  R.options = list(width = 220),
  # viz
  dev.args  = list(bg = 'transparent'),
  dev       = 'svglite',
  fig.align = 'center',
  out.width = '75%',
  fig.asp   = .75,
  # cache
  cache.rebuild = F,
  cache         = F
)

kable_df = function(data, digits=3, ...) {
  kableExtra::kable(
    data,
    digits = digits,
    format = 'html',
    booktabs = T,
    # longtable = F,
    linesep = "",
    ...,
  ) %>% 
    kableExtra::kable_styling(full_width = F)
}

perc = function(x, digits = 1) paste(rnd(x*100, digits = digits), '%')
```

```{r setup-extra, echo=FALSE, eval=TRUE}
xaringanExtra::use_xaringan_extra(
  c(
    "tile_view",
    "animate_css",
    "tachyons",
    'clipboard',
    'fit_screen',
    'webcam',
    'panelset'
  )
)

xaringanExtra::use_logo(
  image_url = 'https://raw.githubusercontent.com/m-clark/m-clark.github.io/master/img/mc_logo.png',
  link_url = 'https://m-clark.github.io',
  width = '5%',
  position =  xaringanExtra::css_position(bottom = "-3em", left = "1em"),
  exclude_class = c("title-slide")
)

xaringanExtra::use_animate_css()

xaringanExtra::style_share_again(
  share_buttons = c("twitter", "linkedin", "pocket")
)
```



```{r load-packs, include=FALSE, eval=TRUE}
library(tidyverse)
library(plotly)
library(visibly)
library(scico)
library(mixedup)
```


---
class: inverse middle center


### **Overview of Mixed Models**

### *More Random Effects*

### *Common Extensions*

### *Issues*

### *Bayesian Approaches*


---
class: inverse middle center animated rollIn rollOut # https://animate.style/


# Getting Started

<br>
<br>
![](img/multilevel1.png)



---
class: inverse smaller50

# Getting Started

.pull-left[
Required for workshop:

```{r}
install.packages('lme4')
```

Others you have already:

- <span class="pack" style = "">nlme</span>
    - included with base R
    
- <span class="pack" style = "">mgcv</span>
    - included with base R
]

.pull-right[
Other recommended (in general)

- <span class="pack" style = "">glmmTMB</span>

- <span class="pack" style = "">rstanarm</span>
    - Bayesian, requires <span class="pack" style = "">rstan</span>
    
- <span class="pack" style = "">brms</span>   
    - Bayesian, requires <span class="pack" style = "">rstan</span>

Misc

- <span class="pack" style = "">mixedup</span>
    - cleaner results for basic models
    - `remotes::install_github('m-clark/mixedup`)
- <span class="pack" style = "">lmerTest</span>
- <span class="pack" style = "">merTools</span>
]

---
class: inverse

# Getting Started: A starting point

Run the following models and summarize.
- Use <span class="func" style = "">summary</span> on the models.
- <span class="objclass" style = "">`sleepstudy`</span> is in the <span class="pack" style = "">lme4</span> package.

```{r eval=TRUE}
library(lme4)

mod_lm = lm(Reaction ~ Days, data = sleepstudy)
mod_lmer = lmer(Reaction ~ Days + (1|Subject), data = sleepstudy)
```

What's the same?

What is different?

---
class: img-slide

<!-- # Getting Started: A starting point -->

```{r eval=TRUE, echo=FALSE, out.width='125%'}
library(brolgar)

sleepstudy_ts = as_tsibble(sleepstudy, key = Subject, index = Days)
 
sleepstudy_ts %>% 
  ggplot(aes(x = Days, y = Reaction, color = Subject)) +
  geom_line() +
  geom_point() +
  geom_smooth(aes(), method = 'lm', se = FALSE) +
  facet_strata(n_strata = 18, nrow = 3, along = Subject) +
  scale_color_scico_d() +
  scale_x_continuous(breaks = c(0, 3, 6, 9)) +
  guides(color = 'none') +
  theme_clean()
```


---
class: inverse

# Getting Started: Terminology

All describe types of what we'd generally call:

*<div class="center">mixed models</div>*

- Variance components
- Random intercepts and slopes
- Random effects
- Random coefficients
- Varying coefficients
- Intercepts- and/or slopes-as-outcomes
- Hierarchical linear models
- Multilevel models
- Growth curve models
- Mixed effects models

---
class: inverse

# Getting Started: Terminology

The 'mixed' part comes from a mixture of:
- *fixed effects*
- *random effects*

Perhaps a better way to phrase:
- *general* 
    - Applies to all observations
- *specific*
    - Applies to a particular group

---
class: inverse

# Getting Started: Clustering

Specific effects come from natural clustering

- Repeated measures
- Spatial/geography
- Social/teams
- Items on a test

---
class: inverse

# Getting Started: GPA Data

Data Description


><span class="" style = "font-size: 75%">For the following we'll assess factors predicting college grade point average (GPA).  Each of the 200 students is assessed for six occasions (each semester for the first three years), so we have observations clustered within students. We have other variables such as job status, sex, and high school GPA.  Some will be in both labeled and numeric form.</span>

```{r}
library(tidyverse)

load('data/gpa.RData')

head(gpa)
```

```{r eval=TRUE, echo = F}
library(tidyverse)

load('../data/gpa.RData')

head(gpa)
```



---
class: inverse

# Getting Started: Standard Regression

Scary formulas! ðŸ‘»

$$\mathscr{gpa} = b_{\mathrm{intercept}} + b_{\mathrm{occ}}\cdot \mathscr{occasion} + \epsilon$$

We have coefficients ( $b$ ) for the intercept and the effect of time.  The error ( $\epsilon$ ) is assumed to be normally distributed with mean 0 and some standard deviation $\sigma$.

$$\epsilon \sim \mathcal{N}(0, \sigma)$$


---
class: inverse

# Getting Started: Standard Regression

Alternatively:

$$\mathscr{gpa} \sim \mathcal{N}(\mu, \sigma)$$

$$\mu = b_{\mathrm{intercept}} + b_{\mathrm{occ}}\cdot \mathscr{occasion}$$
---
class: inverse

# Getting Started: Standard Regression

Run it!

```{r eval=TRUE}
gpa_lm = lm(gpa ~ occasion, data = gpa)
summary(gpa_lm)
```

---
class: inverse

# Getting Started: Single Random Effect

But the observations are clustered
- Not independent
- Correlated within a cluster

--

Regression by cluster?

- Issues:
    - Not easily summarized with many groups
    - Often little data within each cluster 
    - Overfit
    - Ignores what clusters have in common


---
class: img-slide


Think that model is adequate?

```{r echo=FALSE, eval=TRUE, out.width='150%'}
gpa_ts <- as_tsibble(
  gpa,
  key = student,
  index = occasion,
  regular = TRUE
)

gpa_feat = gpa_ts %>% 
  features(gpa, feat_spread)

library(gghighlight)
gpa_ts %>% 
  features(gpa, feat_spread) %>% 
  left_join(gpa_ts) %>% 
  mutate(var_type = NA,
         var_type = case_when(
           var > quantile(var, .95) ~ 'Variable',
           var < quantile(var, .05) ~ 'Constant'
         )) %>% 
  ggplot(aes(x = occasion, y = gpa, color = var_type, group = student)) +
  geom_line() +
  gghighlight(!is.na(var_type), unhighlighted_params = list(size = .1), use_direct_label = F) +
  geom_point(alpha = .5) +
  labs(subtitle = 'Comparing highly variable vs. fairly constant') +
  scale_color_scico_d(end = .6) + 
  # guides(color = guide_colorbar(title = NULL)) +
  theme_clean()
```


---
class: inverse

# Getting Started: Mixed Model

Add a student effect:

$$\mathscr{gpa} = b_{\mathrm{intercept}} + b_{\mathrm{occ}}\cdot \mathscr{occasion} + (\mathrm{effect}_{\mathscr{student}} + \epsilon)$$

Assume the following distribution for it:  


$$\mathrm{effect}_{\mathrm{student}} \sim \mathcal{N}(0, \tau)$$

---
class: inverse

# Getting Started: Mixed Model


If we rearrange it, we can instead focus on model coefficients, rather than as an additional source of error.

$$\mathscr{gpa} = (b_{\mathrm{intercept}} + \mathrm{effect}_{\mathscr{student}}) + b_{\mathrm{occ}}\cdot \mathscr{occasion} +  \epsilon$$
$$\mathscr{gpa} = b_{\mathrm{int\_student}} + b_{\mathrm{occ}}\cdot \mathscr{occasion} +  \epsilon$$

$$b_{\mathrm{int\_student}} \sim \mathcal{N}(b_{\mathrm{intercept}}, \tau)$$

---
class: inverse

# Getting Started: Mixed Model

Another way of showing the mixed model: *multilevel modeling* 
- two part regression model
    - one at the observation level
    - one at the student level

$$\mathrm{gpa} = b_{\mathrm{int\_student}} + b_{\mathrm{occ}}\cdot \mathrm{occasion} + \epsilon$$

$$b_{\mathrm{int\_student}} = b_{\mathrm{intercept}} + \mathrm{effect}_{\mathrm{student}}$$

After 'plugging in' the second level part to the first, it is identical to the previous.


---
class: img-slide

# Getting Started: Application

```{r spaghetti, echo=FALSE, eval=TRUE}
set.seed(1234)
gpa_lm = lm(gpa ~ occasion, data = gpa)
# sample_students = gpa %>% filter(student %in% sample(1:200, 10))
# occasion_sample = gpa$occasion[gpa$student %in% sample_students$student]
# gpa_sample = gpa$gpa[gpa$student %in% sample_students$student]
init = gpa %>%
  modelr::add_predictions(gpa_lm, var = 'all') %>%
  mutate(select = factor(student %in% sample(1:200, 10)),
         sz = c(.5, 1)[select]) %>%
  group_by(student, select) 

init %>%
  ggplot(aes(occasion, gpa)) +
  geom_line(aes(group = student), alpha = .05) +
  geom_line(
    aes(group = student),
    color = '#542437FF',
    data = gpa %>% filter(student %in% sample(student, 10))
  ) +
  geom_smooth(color = '#b20076', method = 'lm', se = F) +
  theme_clean()
```



---
class: inverse code-only

Add the student effect!

```{r gpa-group-lm, echo=FALSE}
gpa_lm_by_group = gpa %>%
  split(.$student) %>%
  map_df( ~ data.frame(t(coef(
    lm(gpa ~ occasion, data = .x)
  )))) %>%
  rename(Intercept = X.Intercept.)

coef_lm = coef(gpa_lm)
```

```{r eval=TRUE}
gpa_mixed = lmer(gpa ~ occasion + (1 | student), data = gpa)

summary(gpa_mixed)
```

---
class: inverse 

# Getting Started: Application

Fixed effects:

```{r eval=T}
extract_fixed_effects(gpa_mixed)
```

Just our standard regression coefficients!
- Interpreted the same

---
class: inverse 

# Getting Started: Application

Variance components:

```{r eval=T}
# lme4::VarCorr(gpa_mixed)
extract_vc(gpa_mixed)
```

These are the standard deviation/variance of the random effects.

The residual SD is akin to that in the `lm` result

---
class: inverse 

# Getting Started: Application

```{r echo = F, eval=T}
extract_vc(gpa_mixed)
```

The proportion of variance attributable to students: ~`r extract_vc(gpa_mixed, ci_level=0)$var_prop[1]*100`%

Can also be interpreted as a correlation.
- Correlation of observations w/in a student
- Called the *intraclass correlation*
- Assumed constant across observations


---
class: inverse 

# Getting Started: Application

These are the random effects for students:

```{r eval=TRUE}
# ranef(gpa_mixed)
extract_random_effects(gpa_mixed)
```

---
class: inverse 

# Getting Started: Application

These are the random coefficients:
- In this case, intercepts
- coef = intercept + effect

```{r eval=TRUE}
# coef(gpa_mixed)
extract_random_coefs(gpa_mixed)
```

---
class: inverse 

# Getting Started: Application

Prediction can include random effects or not.

If they do not, essentially the same as `lm`.

But the random effects will produce better predictions.

```{r eval=T}
predict_no_re = predict(gpa_mixed, re.form=NA)
predict_lm    = predict(gpa_lm)
predict_with_re = predict(gpa_mixed)
```

---
class: img-slide

```{r eval=T, echo=FALSE}
tibble(
  student = as.character(gpa$student),
  lm = predict_lm,
  `lmer no re` = predict_no_re,
  `lmer with re` = predict_with_re
) %>%
  mutate_if(is.numeric, round, digits=2) %>%
  head(30) %>%
  kable_df()
```

---
class: img-slide 

# Getting Started: Application

Which do you think has the better fit? 
- One line for all? Separate for each student?

```{r echo=F, eval=T, out.width='150%'}
init = tibble(
  student = as.character(gpa$student),
  occasion = gpa$occasion,
  gpa = gpa$gpa,
  lm = predict_lm,
  mixed = predict_with_re
) %>%
  filter(student %in% 1:3) %>%
  pivot_longer(lm:mixed, names_to = 'type', values_to = 'prediction')

init %>%
  ggplot(aes(x = occasion)) +
  geom_point(aes(y = gpa, color = student),
             size = 3,
             data = filter(init, type == 'mixed')) +
  geom_line(aes(y = prediction, color = student, group = student), 
    data = filter(init, type == 'lm'),
    color = '#ff5500',
    size = 2
  ) +
  geom_line(aes(y = prediction, color = student, group = student),
            data = filter(init, type == 'mixed')) +
  scale_color_scico_d(begin = .2,
                      end = .9,
                      palette = 'hawaii') +
  labs(subtitle = '') +
  theme_clean()
```

---
class: inverse 

# Getting Started: Cluster level covariates

Thus far we've had only one covariate:
- Time indicator
- Varies with outcome

We can have cluster-level covariates also
- Just add as you would any other

```{r}
lmer(gpa ~ occasion + sex + (1|student), gpa)
```

---
class: inverse 

# Getting Started: Exercises


### Sleep

For this exercise, we'll use the sleep study data from the <span class="pack">lme4</span> package.  The following describes it.

> The average reaction time per day for subjects in a sleep deprivation study. On day 0 the subjects had their normal amount of sleep. Starting that night they were restricted to 3 hours of sleep per night. The observations represent the average reaction time (in milliseconds) on a series of tests given each day to each subject.

---
class: inverse 

# Getting Started: Exercises

After loading the package, the data can be loaded as follows. 

```{r sleepstudy}
library(lme4)
data("sleepstudy")
```

1. Run a regression with Reaction as the target variable and Days as the predictor. 

2. Run a mixed model with a random intercept for Subject.

3. Interpret the variance components and fixed effects.

---
class: inverse 

# Getting Started: Exercises


Rerun the mixed model with the GPA data adding the cluster level covariate of `sex`, or high school GPA (`highgpa`), or both.  Interpret all aspects of the results.

```{r gpa_cluster, echo=F, eval=FALSE}
gpa_mixed_cluster_level = lmer(gpa ~ occasion + sex + highgpa + (1|student), gpa)
summary(gpa_mixed_cluster_level)
```

What happened to the student variance after adding cluster level covariates to the model?



